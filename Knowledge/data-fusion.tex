\section{Data Integration in Distributed Sensor Networks}

\index{information integration} 
\index{distributed sensor network}
Distributed sensor networks represent  very large distributed systems that are becoming omnipresent in current times. They
consist of large number of sensor nodes, with constrained processing power, memory and network connectivity. The sensors
read different environmental data or process parameters based on the application needs, and forward them to computers with 
adequate processing power for further processing. 
%
Typically, a sensor network is organized hierarchically as shown in figure~\ref{fig:knowledge:iot}. The lowest layer in the 
hierarchy generally comprises numerous inexpensive devices, with little processing power and memory (~5 -- 50kB), and which 
need to conserve power to have a reasonable battery life. Often, a redundant set of sensors are deployed for achieving
fault-tolerance. These devices are connected to set of gateways with low-power lossy wireless networks. Typically, the gateway 
nodes deploy commodity processors, such as laptops or mobile handsets, which in turn, are connected to the cloud servers with 
wired or wireless IP networks.  Sometimes, several sensors are connected to a relatively powerful processor, forming a smart 
object. 

\begin{figure}[!htbp]
	\centerline{
		\epsfig{figure=./Knowledge/iot.eps,width=0.8\linewidth}
	}
	\caption{Typical distributed sensor network architecture}
	\label{fig:knowledge:iot}
\end{figure}

\index{sensor}
The sensors can be of many kinds, and they can transmit data asynchronously in many different formats with different data rates. 
For example, a temperature sensor may transmit a vector of few integers every few hours (or when requested), while a video camera 
may transmit a live video stream with a bitrate of a few Mbps. Further, the term ``sensor'' can be broadly used to denote anything 
that senses, including human sensors, who may report events that they observe in text or image format. An example application
of human sensors is citizens reporting traffic violation data to the police department portal, which complements data collected
from street-corner cameras. 
%
\index{edge computing} \index{dew computing} \index{fog computing}
Despite the constrained setup, many of the systems need to integrate and interpret environmental data, and respond to the
environmental changes in real time. Thus, distributed sensor systems put several challenges to knowledge-based data integration. 
It is quite apparent that all data generated by the sensors cannot be uploaded to the cloud for analysis. Thus, these systems 
call for a hierarchical and incremental approach for distributed data integration. The processing at the lower levels of hierarchy 
in such distributed systems is known as {\em edge}, {\em dew} or {\em fog} computing depending on the number and the nature of 
the devices connected, and the platform where such processing is performed. We have already seen an example of stream data 
processing in constrained systems in chapter~\ref{chap:bigdata}. In this section, we present a few other approaches commonly 
followed in data integration in distributed sensor networks.

\subsection{Sensor Data Integration}

\index{semantic sensor network|see {SSN}} \index{SSN|textbf}
\index{semantic sensor web|see {SSN}} 
\index{sensor}
\index{ontology} \index{RDF resource}
{\em Semantic Sensor Network} (SSN) (also called semantic sensor web) represents a combination of distributed sensor network and 
semantic web technologies. The sensors and their observations are encoded with the knowledge description languages in semantic web 
technologies, which enables more expressive representation, semantic access, and formal analysis of the sensor resources. For 
example, a data sensor is viewed as an RDF resource in SSN, and be characterized by an IRI, when an appropriate SPARQL query may 
return the latest sensor reading. A user may not be aware with the existing sensors in the field (there may be numerous ones) and
their IRIs, but may need to select it logically in a specific problem context. For example, a user may be interested in the current 
temperature at a certain city, or may like to see the traffic video at a certain street-corner. The issue is to relate the logical 
specification to a certain sensor IRI.

\begin{figure}[!htbp]
	\centerline{
		\epsfig{figure=./Knowledge/ssn-model.eps,width=0.8\linewidth}
	}
	\caption{Stimulus-Sensor-Observation pattern: model for SSN ontology (\basedon{~\citep{Compton:2012})}}
	\label{fig:knowledge:ssn-model}
\end{figure}

\index{SSN ontology} \index{DUL ontology framework}
The SSN ontology~\citep{Compton:2012} is an ontology that can be used to describe various aspects of a sensor and it's deployment
status, including the sensor properties, such as what it measures, it's accuracy, it's operating range, it's environmental
limitations, it's deployment status, etc. For example, in a harsh weather condition, one may need to select a sensor with a 
suitable range though it may be less accurate. Similarly, after sunset, we may need to choose a night-vision camera with a lower 
resolution, and without color recognition capability. The SSN ontology has been developed with the basic stimulus-sensor-observation 
pattern shown in figure~\ref{fig:knowledge:ssn-model}. The core SSN ontology developed over a lightweight ontology framework,
DUL~\citep{Scherp:2009}, with 10 classes and 16 properties.

\index{DBpedia} \index{interval algebra}
The sensor ontology often need to be complemented with more information, which are not specific to sensor but are related to it, such as 
location ontology, feature ontology (specifying relations between the locations and features respectively), for practical applications.
For example, to discover the temperature sensors located in major cities of India, DBpedia can be consulted to map the Indian
cities to their geolocations (latitude, longitude), and temperature sensors within a certain vicinity those geolocations can be 
selected using interval algebra (see chapter~\ref{chap:event-ordering}). 
More complex applications can be developed by integrating domain-specific ontologies in the system. For example, systems for smart 
water and traffic management systems using multiple domain ontologies have been reported in~\citep{Goel:2017:1,Goel:2017:2}.
%
In summary, ontology is useful in sensor based applications in several respects~\citep{Taylor:2011}, namely:
\begin{enumerate}
	\item Discover and assert the state of the sensors that can be used in a query context.
	\item Develop a formal specification for the event of interest in context of the current query and the usable sensors.
	\item Reuse measurements that are already being made (as a result of an earlier query, or that are routinely taken) wherever 
		possible.
	\item Actuate the sensor devices to make the measurements that cannot be reused.
	\item Develop a formal specification of actions to be taken if the event of interest is detected. 
\end{enumerate}

\section{Data Integration in Constrained Systems}
\label{sec:knowledge:constrained}

\index{information integration!in constrained systems}
\index{Bayesian data integration} \index{incremental data integration}
\index{emergent information}
While the sensors in a distributed sensor network report raw sensory data, the inference in a system generally requires integration
of data from multiple system. At the lowest level, it may take the form of finding a robust estimate of an environmental parameter,
such as temperature, by aggregating data from the readings of a number of inaccurate and possibly faulty thermometers. At a higher
level, the inference may result in a state of an emergent variable, like the possibility of rainfall at allocation, by aggregating the 
weather data from nearby places. In a distributed system without constraints, where there are powerful processors and adequate network 
bandwidth, all data could be transmitted to a central server for their aggregation and inferencing. This approach is not feasible in 
the constrained systems, because of the data volume and network bandwidth. 
%
In the following text, we present a Bayesian approach for distributed and incremental integration of sensor data~\citep{Makareno:2009}. 
As an application example, we consider a simple example where the state of a process, for example the location of a moving target, is 
being measured by a set of distributed sensors each connected to a different processor. Data from each of the sensors can be noisy, and 
the task is to estimate the current state $x$ of the process, from a set of $n$ sensor readings $\mathbf{z} = \{ z_1, z_2, \dots, 
z_n \}$. In general, the raw sensor readings may have to go through some transformations before integration, for example 
conversion of coordinate system in the current example. We assume that the variables $z_i$s incorporate the necessary transformations.

In Bayesian formulation, a process parameter $x$ can be estimated from a set of noisy observations $\mathbf{z}$ as a probability density
function (pdf) using the the formula
\begin{equation}
	p( x \mid \mathbf{z} ) = \frac{p(x).P(\mathbf{z \mid x})}{P(\mathbf{z})}
	\label{eqn:knowledge:bayesian1}
\end{equation}

\noindent
The left-hand side of the equation represents the {\em posterior} probability density function of $x$, given the set of observations
$\mathbf{z}$. 
%
The first term in the numerator of the right-hand side represents the {\em prior} pdf of $x$, and the second 
term represents the {\em conditional} probability of observing the signals $\mathbf{z}$ when the process parameter assumes a value 
$x$. We have used a convention for use of $P(\circ)$ (upper case) to represent probability value and $p(\circ)$ (lower case) 
to represent probability density throughout this text. 
%
The denominator in the right-hand side is the {\em marginal} probability for the set of signals $\mathbf{z}$ to occur, and is given by
\begin{equation}
	P(\mathbf{z}) = \int_x p( \mathbf{z} | x )dx
\end{equation}
\noindent
where the integration ranges over the entire range of $x$. Note that the denominator does not depend on the process parameter $x$
and is generally regarded as a normalizing constant $\kappa$, which makes $\int_x p(x \mid \mathbf{z}).dx = 1$. 
%
Further, assuming the sensor readings $\{z_1, z_2, \dots, z_n\}$ to be independent of each other, we have
\begin{equation}
	P(\mathbf{z} \mid x) = P(z_1 \mid x).P(z_2 \mid x). \dots .P(z_n \mid x)
\end{equation}

\noindent
With these substitutions, we can rewrite equation~\ref{eqn:knowledge:bayesian1} as
\begin{equation}
	p( x \mid \mathbf{z} ) = \frac{1}{\kappa}. p(x).P(z_1 \mid x).P(z_2 \mid x). \dots .P(z_n \mid x)
	\label{eqn:knowledge:bayesian2}
\end{equation}

\noindent
The prior pdf $p(x)$ in equation~\ref{eqn:knowledge:bayesian2} is generally based on some specific model of the physical process,
e.g. a prediction model based on the earlier locations of the target in a tracking problem (e.g. Kalman Filter~\citep{Welch:1995}).
The conditional probabilities $p(z_i \mid x)$ are known from the calibration data of the sensors. We need not worry about
computation of the constant $\kappa$, since it can always be found by normalizing the pdf.

In a centralized architecture, all the sensor readings $z_1, \dots z_n$ could be collected on a central server and the process
parameter $x$ can be estimated using equation~\ref{eqn:knowledge:bayesian2}. However, the decomposition of the expression in the product 
form on the right-hand side provides an opportunity of incremental and distributed integration. The guiding principle is that
if $p(x \mid \mathbf{z} \setminus z_i)$ is known, then $p(x \mid \mathbf{z})$ can be computed as
\begin{equation}
	p( x \mid \mathbf{z} ) 
		= \frac{1}{\kappa}p(x \mid \mathbf{z}\setminus z_i).P(z_i \mid x)
	\label{eqn:knowledge:bayesian3}
\end{equation}

\noindent
This result can be interpreted as the posterior pdf for $x$ from earlier observations serves as the prior for a later observation.
As an example, consider three processing nodes $A$, $B$ and $C$, each equipped with a camera, and are tasked to collaboratively track
a target. At the end of the computation cycle, each of the nodes should have an estimate of the state of the target. Denoting the sensor 
readings from the three sensors as $z_a$, $z_b$ and $z_c$ respectively, the posterior estimate pdf of $x$ is given by
\begin{equation}
	p( x \mid z_a, z_b, z_c) = \frac{1}{\kappa}. p(x). P(z_a|x). P(z_b|x). P(z_c|x)
\end{equation}

\begin{figure}[!htbp]
	\centerline{
		\epsfig{figure=./Knowledge/bayesian.eps,width=0.8\linewidth}
	}
	\caption{Bayesian data fusion in distributed system (\basedon{\citep{Makareno:2009}})}
	\label{fig:knowledge:bayesian}
\end{figure}
 
\noindent
\index{gossip protocol}
Let us now assume that the three nodes are interconnected as shown in figure~\ref{fig:knowledge:bayesian}, and they communicate
using some gossip protocol (see chapter~\ref{chap:gossip}). The links allow a node to communicate it's own estimates to it's 
neighbors. The neighbors, in turn, use these estimates to revise their own estimates and communicate to their respective neighbors. 
%
\index{channel filter}
In order to collaborate, each of the nodes maintain a {\em channel filter} towards the other connected nodes.
A channel filter $\phi_{ij}$ contains the current estimate of $x$ at node $i$, which is communicated to node $j$. Initially, all
the channel filters are loaded with the initial estimate of $p(x)$, i.e. in this example, 
$\phi_{ba} = \phi_{ab} = \phi_{cb} = \phi_{bc} = p(x)$.
%
Now, the processors asynchronously compute their own estimates of $x$ based on it's local sensor values, when the local estimates 
at the three processors are given by

\begin{equation}
	x_a = \frac{1}{\kappa_a} p(x).P(z_a|x), \hspace{5mm} 
	x_b = \frac{1}{\kappa_b} p(x).P(z_b|x) \hspace{5mm} \text{and} \hspace{5mm}
	x_c = \frac{1}{\kappa_c} p(x).P(z_c|x)  
\end{equation}

\noindent
Once $A$ has updated it's estimate for $x$, it sends a message to $B$ communicating the updated estimate. Consequently, the channel 
filter gets updated as $\phi_{ab}^* = x_a$,  and node $B$ revises it's estimate of $x$ as

\begin{equation}
	x_b^* 	= \frac{\phi_{ab}^*}{\phi_{ab}}. x_b 
		= \frac{x_a.x_b}{p(x)}
		= \frac{1}{\kappa_1} p(x).P(z_a|x).P(z_b|x)
		= p(x \mid z_a,z_b)
\end{equation}
\noindent
The right-hand side of the equation is the updated value of $x$ as the result of combined observations $z_a$ and $z_b$. Similarly,
when $C$ update it's estimate for $x$, it also sends a message to $B$, when the channel filter $\phi_{cb}$ is updated to 
$\phi_{cb}^* = x_c$, and the node $B$ further revises it's estimate for $x$ as
\begin{equation}
	x_b^{**}= \frac{\phi_{cb}^*}{\phi_{cb}}. x_{ab} 
		= \frac{x_b^*.x_c}{p(x)}
		= \frac{1}{\kappa_2} p(x).P(z_a|x).P(z_b|x).P(z_c|x)
		= p(x, \mid z_a,z_b,z_c)
\end{equation}
\noindent
Thus, the estimate at $B$ is updated with all the observations $a$, $b$ and $c$. Whenever node $B$ updates it's estimate, it sends a
message to it's neighbors $A$ and $C$, and the estimates at those nodes are also eventually updated to $p(x \mid z_a, z_b, z_c)$.
%
In this method of integrated data fusion, we note that
\begin{enumerate}
	\item Data integration from a number of (inaccurate) sensors results in robust estimation.  
	\item None of the nodes need to hold the entire observation set at any point of time and fuses data incrementally. This 
		allows data integration in constrained environment possible. However, the price paid is a longer computation delay.
	\item The processor nodes need to exchange short messages. 
	\item It is easy to verify that the sequence of events (measurements conducted by the sensors) and the messages does not 
		matter. All the processors may work asynchronously and they eventually converge to the same estimate.
	\item When the system state dynamically changes (e.g. in a tracking scenario) 
		\begin{itemize}
			\item The sensors repeatedly measure the parameters at certain intervals, and the system states at all
				of the nodes are periodically updated. 
			\item The measurements on the different nodes can be asynchronous, and can have different periodicities. 
			\item The system is robust against message loss, which can be expected in a lossy WPAN environment. The 
				intended recipient of a lost message catches up with the next message.
		\end{itemize}
\end{enumerate}

\index{emergent information}
In this Bayesian framework of data-fusion, $x$ need not be a simple aggregate of the sensory data $z_i$, but can be an emergent
knowledge entity. In general, the state variable $x$ and the sensor data $z_i$ need not be of the same kind. In the given example, 
(though we have not said it so far) $x$ generally represents a 3D location and pose of the target, while $z_i$s may be the 
2D projection data of the target on the screen of the sensing cameras. Thus, the inferred variable is an emergent information about 
the system and not just and aggregate of the acquired data. 

\index{heterogeneous data integration}
\index{semantic data integration}
In the Bayesian framework of data integration, the data from different sensors can be heterogeneous in nature. For example, the state 
variable $x$ may denote the probability of rainfall at a location, while the variables $z_i$s may mean various weather parameters, such 
as humidity, temperature, cloudiness and wind speed, at that location and some nearby locations. The framework can also operate at 
different semantic levels. All data items that are being integrated need not necessarily be raw sensory data. Some or all of the data 
items can be abstracted data, such as machine annotated satellite images, or human observations. This flexibility enables integration 
of heterogeneous data, such as sensor data, machine interpreted domain-specific data and crowd-sourced data at various semantic
levels in the same framework. 

\index{semantic data integration!optimization}
The Bayesian framework also enables optimization in the volume of data that is to be used for inferencing the state of a system. 
An IoT system is generally characterized by a large redundancy in the sensors, when robust inferencing may not need processing 
of all sensory data. If there is sufficient confidence in the result, as measured by the peakiness of the posterior pdf, after 
processing of a subset of sensory data, further integration can be discontinued~\citep{Ghosh:2004}.


